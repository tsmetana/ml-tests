{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common code\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "class LogFile:\n",
    "    __path = None\n",
    "    __hostname_pattern = re.compile('^[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$')\n",
    "    __id_suffix_pattern = re.compile('^([a-z]+-)+[a-f0-9]+-.{5}$') # something-and-else-fb6afe71-ea3fd\n",
    "    __id_prefix_pattern1 = re.compile('^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}-(.*)$') # fb6afe71-6f53-48dd-920f-743ab83e1c2f-something\n",
    "    __id_prefix_pattern2 = re.compile('^[a-f0-9]{32}-(.*)$') # 32-digit hex number prefix\n",
    "    __id_suffix_pattern3 = re.compile('^([a-z]+-)+[a-f0-9]{5}$') # something-else-ea3fd\n",
    "    __id_suffix_pattern4 = re.compile('^([a-z]+-)+[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$') # something-16258d4f-7f2d-47f2-a11a-ef10e66dfc12\n",
    "    __id_token_pattern1 = re.compile('^[a-z-]+-') # just words separated by dashes\n",
    "    __id_token_pattern2 = re.compile('^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$') # 16258d4f-7f2d-47f2-a11a-ef10e66dfc12\n",
    "    __numeric_token_pattern = re.compile('^[+]?[0-9]+[.]*[0-9]*[mug]*i*[sb]*$') # +4.653ms and such\n",
    "    __hex_num_pattern = re.compile('[a-z0-9]+')\n",
    "    __ttable = str.maketrans('{}[]()=:/\\\\,&?_', '              ', '\"\\'')\n",
    "    __db = None\n",
    "    __dbc = None\n",
    "    \n",
    "    def __init__(self, path = None, db_path = None, clear = True):\n",
    "        if db_path is None:\n",
    "            self.__db = sqlite3.connect(\":memory:\")\n",
    "        else:\n",
    "            self.__db = sqlite3.connect(db_path)\n",
    "        self.__dbc = self.__db.cursor()\n",
    "        self.__dbc.execute(\"create table if not exists log_lines(ID integer primary key, line text not NULL)\")\n",
    "        self.__dbc.execute(\"create table if not exists tokens(ID integer primary key, token text unique not NULL, freq integer not null default 1)\")\n",
    "        self.__dbc.execute(\"create table if not exists identifiers(ID integer primary key, identifier text unique not NULL)\")\n",
    "        self.__dbc.execute(\"create table if not exists log_line_tokens(line_ID integer not null, token_ID integer not null, primary key (line_ID, token_ID))\")\n",
    "        self.__dbc.execute(\"create table if not exists log_line_identifiers(line_ID integer not null, identifier_ID integer not null, primary key (line_ID, identifier_ID))\")\n",
    "        if clear:\n",
    "            self.__dbc.execute(\"delete from log_lines\") # this should make the class idempotent but allows only for one log file to be processed\n",
    "        self.set_path(path)\n",
    "\n",
    "    def __fini__(self):\n",
    "        self.__db.close()\n",
    "\n",
    "    def __is_hostname(self, s):\n",
    "        return s.count('.') > 0 and self.__hostname_pattern.match(s)\n",
    "\n",
    "    def __is_hash(self, s):\n",
    "        if (len(s) == 32 or len(s) == 64) and self.__hex_num_pattern.match(s):\n",
    "            return True\n",
    "        if self.__id_token_pattern2.match(s):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def __is_numberlike(self, s):\n",
    "        if s.isnumeric() or self.__numeric_token_pattern.match(s):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # 'some-operator-69854dc866-sbpwz' > 'some-operator-id_suffix'\n",
    "    def __replace_id_suffix(self, s):\n",
    "        m = None\n",
    "        if s.startswith(\"--\"):\n",
    "            return(s, False)\n",
    "        if self.__id_suffix_pattern.match(s):\n",
    "            m = self.__id_token_pattern1.match(s)\n",
    "        if m != None:\n",
    "            return (m.group() + 'id_suffix', True)\n",
    "        m = self.__id_suffix_pattern4.match(s)\n",
    "        if m is not None:\n",
    "            return (m.group(1) + 'id_suffix', True)\n",
    "        return (s, False)\n",
    "\n",
    "    def __replace_id_prefix(self, s):\n",
    "        m = self.__id_prefix_pattern1.match(s)\n",
    "        if m is not None:\n",
    "            return ('id-prefix-' + m.group(1), True)\n",
    "        m = self.__id_prefix_pattern2.match(s)\n",
    "        if m is not None:\n",
    "            return ('id-prefix-' + m.group(1), True)\n",
    "        return (s, False)\n",
    "        \n",
    "    def __log_line_tokenize(self, line):\n",
    "        ret = []\n",
    "        tokens = line.split(\" \", 8)\n",
    "        # Filter out things that can't be possibly kubernetes and even the k8s startup log lines\n",
    "        if not (tokens[4].startswith(\"kubenswrapper\") and (tokens[5].startswith(\"i\") or tokens[5].startswith(\"w\") or tokens[5].startswith(\"e\"))):\n",
    "             raise IndexError\n",
    "        identifiers = []\n",
    "        log_text = tokens.pop(-1)\n",
    "        log_text = log_text.translate(self.__ttable)\n",
    "        log_text_tokens = log_text.split()\n",
    "        i = 0\n",
    "        for t in log_text_tokens:\n",
    "            log_text_tokens[i] = t.strip(\". ?!*#,+\")\n",
    "            i = i + 1\n",
    "        tokens = [tokens[5]]\n",
    "        tokens[0] = tokens[0][0] + '000'\n",
    "        tokens = tokens + log_text_tokens\n",
    "        for t in tokens:\n",
    "            tr = t\n",
    "            if self.__is_numberlike(t):\n",
    "                continue\n",
    "            if self.__is_hostname(t):\n",
    "                tr = \"_hostname_token\"\n",
    "            elif self.__is_hash(t):\n",
    "                if not t in identifiers:\n",
    "                    identifiers.append(t)\n",
    "                tr = \"_hash_token\"\n",
    "            else:\n",
    "                (tr, found) = self.__replace_id_prefix(t)\n",
    "                if found and not t in identifiers:\n",
    "                    identifiers.append(t)\n",
    "                (tr, found) = self.__replace_id_suffix(tr)\n",
    "                if found and not t in identifiers:\n",
    "                    identifiers.append(t)\n",
    "            ret.append(tr)\n",
    "        return (ret, identifiers)\n",
    "    \n",
    "    def set_path(self, path):\n",
    "        self.__path = path\n",
    "\n",
    "    def parse_log(self, limit = 0):\n",
    "        logfile = open(self.__path, \"r\")\n",
    "        line = logfile.readline().casefold()\n",
    "        line_num = 1 # this will become line_ID\n",
    "        while (len(line) > 0 and limit == 0) or (len(line) > 0 and limit != 0 and line_num <= limit):\n",
    "            try:\n",
    "                tokens, identifiers = self.__log_line_tokenize(line)\n",
    "                self.__dbc.execute(\"insert into log_lines (ID, line) values (?, ?)\", (line_num, line))\n",
    "                l_id = self.__dbc.lastrowid\n",
    "                for t in tokens:\n",
    "                    if t.isspace():\n",
    "                        continue\n",
    "                    self.__dbc.execute(\"insert into tokens (token) values (?) on conflict do update set freq = freq + 1\", (t,))\n",
    "                    t_id = self.__dbc.execute(\"select id from tokens where (token = ?)\", (t,)).fetchone()[0]\n",
    "                    self.__dbc.execute(\"insert or ignore into log_line_tokens (line_ID, token_ID) values (?,?)\", (l_id, t_id))\n",
    "                for t in identifiers:\n",
    "                    self.__dbc.execute(\"insert or ignore into identifiers (identifier) values (?)\", (t,))\n",
    "                    i_id = self.__dbc.execute(\"select id from identifiers where (identifier = ?)\", (t,)).fetchone()[0]\n",
    "                    self.__dbc.execute(\"insert or ignore into log_line_identifiers (line_ID, identifier_ID) values (?,?)\", (l_id, i_id))\n",
    "            except IndexError:\n",
    "                pass\n",
    "            line = logfile.readline().casefold()\n",
    "            line_num = line_num + 1\n",
    "        self.__db.commit()\n",
    "        logfile.close()\n",
    "\n",
    "    def export_freq_table(self, filename):\n",
    "        with open(filename, \"w\") as fw:\n",
    "            for r in self.__dbc.execute(\"select token, freq from tokens\"):\n",
    "                fw.write(f\"\\\"{r[0]}\\\", {r[1]}\\n\")\n",
    "\n",
    "    def export_words_table(self, filename):\n",
    "        self.__db.commit()\n",
    "        self.__dbc.execute(f\"attach database '{filename}' as words_db\")\n",
    "        self.__dbc.execute(\"create table if not exists words_db.words (id integer primary key, word text unique not null)\")\n",
    "        self.__dbc.execute(\"insert or ignore into words_db.words (word) select token from tokens\")\n",
    "        self.__db.commit()\n",
    "    \n",
    "    def import_words_table(self, filename):\n",
    "        r = self.__dbc.execute(\"create table if not exists words (id integer primary key, word text unique not null)\")\n",
    "        self.__dbc.execute(\"delete from words\")\n",
    "        with open(filename, \"r\") as fr:\n",
    "            for line in fr:\n",
    "                self.__dbc.execute(\"insert or ignore into words (word) values (?)\",(line.rstrip(),))\n",
    "        self.__db.commit()\n",
    "\n",
    "    def get_words_count(self):\n",
    "        r = self.__dbc.execute(\"select count(*) from words\")\n",
    "        return r.fetchone()[0]\n",
    "\n",
    "    def save_db(self, filename):\n",
    "        self.__db.commit()\n",
    "        self.__dbc.execute(f\"vacuum into '{filename}'\")\n",
    "\n",
    "    def db_handle(self):\n",
    "        return self.__db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the raw dictionary with token counts\n",
    "#logfile = LogFile(\"journal-aws-efs-operator-e2e.0\", 'efs-e2e.db')\n",
    "logfile = LogFile(\"journal-aws-efs-operator-e2e.0\")\n",
    "logfile.parse_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = logfile.db_handle()\n",
    "cur = con.cursor()\n",
    "r = cur.execute(\"select count(ID) from log_lines\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from log_lines limit 20\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cur.execute(\"select count(ID) from tokens\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from tokens order by freq desc limit 20\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile.export_freq_table(\"frequencies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cur.execute(\"select count(id) from identifiers\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from identifiers\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cur.execute(\"select count(line_ID) from log_line_identifiers\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from log_line_identifiers order by line_ID limit 20\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cur.execute(\"select count(line_ID) from log_line_tokens\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from log_line_tokens order by token_ID limit 20\"):\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile.export_words_table('words.db')\n",
    "r = cur.execute(\"create table words (id integer primary key, word text unique not null)\")\n",
    "\n",
    "# Here: review the db / words table and export as csv to be imported as the label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "logfile = LogFile(\"journal-aws-efs-operator-e2e.0\", 'test2.db')\n",
    "logfile.import_words_table('words.csv')\n",
    "logfile.parse_log()\n",
    "\n",
    "con = logfile.db_handle()\n",
    "cur = con.cursor()\n",
    "\n",
    "words_num = logfile.get_words_count()\n",
    "print(f\"Dictitonary size: {words_num} words\")\n",
    "\n",
    "prev_identifier_id = 0\n",
    "prev_line_id = 0\n",
    "raw_dataset = []\n",
    "line_words = torch.zeros(words_num)\n",
    "raw_dataset.append(line_words)\n",
    "for r in cur.execute(\"select words.id as word_id, log_line_identifiers.identifier_ID, log_line_identifiers.line_ID from words \\\n",
    "        join tokens on tokens.token = words.word \\\n",
    "            join log_line_tokens on log_line_tokens.token_ID = tokens.ID \\\n",
    "                join log_line_identifiers on  log_line_identifiers.line_ID = log_line_tokens.line_ID \\\n",
    "                        order by log_line_identifiers.identifier_ID, log_line_identifiers.line_ID, word_id\"):\n",
    "    (word_id, identifier_id, line_id) = r\n",
    "    if line_id != prev_line_id:\n",
    "        raw_dataset.append(line_words)\n",
    "        line_words = torch.zeros(words_num)\n",
    "        prev_line_id = line_id\n",
    "    line_words[word_id - 1] = 1.0\n",
    "training_set = torch.stack(raw_dataset)\n",
    "\n",
    "print(training_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, io_dim, h_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(io_dim, h_dim)\n",
    "        self.linear = nn.Linear(h_dim, io_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,_ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
