{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common code\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "class LogFile:\n",
    "    __path = None\n",
    "    __hostname_pattern = re.compile('^[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$')\n",
    "    __id_suffix_pattern = re.compile('^([a-z]+-)+[a-f0-9]+-.{5}$') # something-and-else-fb6afe71-ea3fd\n",
    "    __id_prefix_pattern1 = re.compile('^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}-(.*)$') # fb6afe71-6f53-48dd-920f-743ab83e1c2f-something\n",
    "    __id_prefix_pattern2 = re.compile('^[a-f0-9]{32}-(.*)$') # 32-digit hex number prefix\n",
    "    __id_suffix_pattern3 = re.compile('^([a-z]+-)+[a-f0-9]{5}$') # something-else-ea3fd\n",
    "    __id_suffix_pattern4 = re.compile('^([a-z]+-)+[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$') # something-16258d4f-7f2d-47f2-a11a-ef10e66dfc12\n",
    "    __id_token_pattern1 = re.compile('^[a-z-]+-') # just words separated by dashes\n",
    "    __id_token_pattern2 = re.compile('^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$') # 16258d4f-7f2d-47f2-a11a-ef10e66dfc12\n",
    "    __numeric_token_pattern = re.compile('^[+]?[0-9]+[.]*[0-9]*[mug]*i*[sb]*$') # +4.653ms and such\n",
    "    __hex_num_pattern = re.compile('[a-z0-9]+')\n",
    "    __ttable = str.maketrans('{}[]()=:/\\\\,&?_', '              ', '\"\\'')\n",
    "    __db = None\n",
    "    __dbc = None\n",
    "    \n",
    "    def __init__(self, path = None, db_path = None, clear = True):\n",
    "        if db_path is None:\n",
    "            self.__db = sqlite3.connect(\":memory:\")\n",
    "        else:\n",
    "            self.__db = sqlite3.connect(db_path)\n",
    "        self.__dbc = self.__db.cursor()\n",
    "        self.__dbc.execute(\"create table if not exists log_lines(ID integer primary key, line text not NULL)\")\n",
    "        self.__dbc.execute(\"create table if not exists tokens(ID integer primary key, token text unique not NULL, freq integer not null default 1)\")\n",
    "        self.__dbc.execute(\"create table if not exists identifiers(ID integer primary key, identifier text unique not NULL)\")\n",
    "        self.__dbc.execute(\"create table if not exists log_line_tokens(line_ID integer not null, token_ID integer not null, primary key (line_ID, token_ID))\")\n",
    "        self.__dbc.execute(\"create table if not exists log_line_identifiers(line_ID integer not null, identifier_ID integer not null, primary key (line_ID, identifier_ID))\")\n",
    "        if clear:\n",
    "            self.__dbc.execute(\"delete from log_lines\") # this should make the class idempotent but allows only for one log file to be processed\n",
    "        self.set_path(path)\n",
    "\n",
    "    def __fini__(self):\n",
    "        self.__db.close()\n",
    "\n",
    "    def __is_hostname(self, s):\n",
    "        return s.count('.') > 0 and self.__hostname_pattern.match(s)\n",
    "\n",
    "    def __is_hash(self, s):\n",
    "        if (len(s) == 32 or len(s) == 64) and self.__hex_num_pattern.match(s):\n",
    "            return True\n",
    "        if self.__id_token_pattern2.match(s):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def __is_numberlike(self, s):\n",
    "        if s.isnumeric() or self.__numeric_token_pattern.match(s):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # 'some-operator-69854dc866-sbpwz' > 'some-operator-id_suffix'\n",
    "    def __replace_id_suffix(self, s):\n",
    "        m = None\n",
    "        if s.startswith(\"--\"):\n",
    "            return(s, False)\n",
    "        if self.__id_suffix_pattern.match(s):\n",
    "            m = self.__id_token_pattern1.match(s)\n",
    "        if m != None:\n",
    "            return (m.group() + 'id_suffix', True)\n",
    "        m = self.__id_suffix_pattern4.match(s)\n",
    "        if m is not None:\n",
    "            return (m.group(1) + 'id_suffix', True)\n",
    "        return (s, False)\n",
    "\n",
    "    def __replace_id_prefix(self, s):\n",
    "        m = self.__id_prefix_pattern1.match(s)\n",
    "        if m is not None:\n",
    "            return ('id-prefix-' + m.group(1), True)\n",
    "        m = self.__id_prefix_pattern2.match(s)\n",
    "        if m is not None:\n",
    "            return ('id-prefix-' + m.group(1), True)\n",
    "        return (s, False)\n",
    "        \n",
    "    def __log_line_tokenize(self, line):\n",
    "        ret = []\n",
    "        tokens = line.split(\" \", 8)\n",
    "        # Filter out things that can't be possibly kubernetes and even the k8s startup log lines\n",
    "        if not (tokens[4].startswith(\"kubenswrapper\") and (tokens[5].startswith(\"i\") or tokens[5].startswith(\"w\") or tokens[5].startswith(\"e\"))):\n",
    "             raise IndexError\n",
    "        identifiers = []\n",
    "        log_text = tokens.pop(-1)\n",
    "        log_text = log_text.translate(self.__ttable)\n",
    "        log_text_tokens = log_text.split()\n",
    "        i = 0\n",
    "        for t in log_text_tokens:\n",
    "            log_text_tokens[i] = t.strip(\". ?!*#,+\")\n",
    "            i = i + 1\n",
    "        tokens = [tokens[5]]\n",
    "        tokens[0] = tokens[0][0] + '000'\n",
    "        tokens = tokens + log_text_tokens\n",
    "        for t in tokens:\n",
    "            tr = t\n",
    "            if self.__is_numberlike(t):\n",
    "                continue\n",
    "            if self.__is_hostname(t):\n",
    "                tr = \"_hostname_token\"\n",
    "            elif self.__is_hash(t):\n",
    "                if not t in identifiers:\n",
    "                    identifiers.append(t)\n",
    "                tr = \"_hash_token\"\n",
    "            else:\n",
    "                (tr, found) = self.__replace_id_prefix(t)\n",
    "                if found and not t in identifiers:\n",
    "                    identifiers.append(t)\n",
    "                (tr, found) = self.__replace_id_suffix(tr)\n",
    "                if found and not t in identifiers:\n",
    "                    identifiers.append(t)\n",
    "            ret.append(tr)\n",
    "        return (ret, identifiers)\n",
    "    \n",
    "    def set_path(self, path):\n",
    "        self.__path = path\n",
    "\n",
    "    def parse_log(self, limit = 0):\n",
    "        logfile = open(self.__path, \"r\")\n",
    "        line = logfile.readline().casefold()\n",
    "        line_num = 1 # this will become line_ID\n",
    "        while (len(line) > 0 and limit == 0) or (len(line) > 0 and limit != 0 and line_num <= limit):\n",
    "            try:\n",
    "                tokens, identifiers = self.__log_line_tokenize(line)\n",
    "                self.__dbc.execute(\"insert into log_lines (ID, line) values (?, ?)\", (line_num, line))\n",
    "                l_id = self.__dbc.lastrowid\n",
    "                for t in tokens:\n",
    "                    if t.isspace():\n",
    "                        continue\n",
    "                    self.__dbc.execute(\"insert into tokens (token) values (?) on conflict do update set freq = freq + 1\", (t,))\n",
    "                    t_id = self.__dbc.execute(\"select id from tokens where (token = ?)\", (t,)).fetchone()[0]\n",
    "                    self.__dbc.execute(\"insert or ignore into log_line_tokens (line_ID, token_ID) values (?,?)\", (l_id, t_id))\n",
    "                for t in identifiers:\n",
    "                    self.__dbc.execute(\"insert or ignore into identifiers (identifier) values (?)\", (t,))\n",
    "                    i_id = self.__dbc.execute(\"select id from identifiers where (identifier = ?)\", (t,)).fetchone()[0]\n",
    "                    self.__dbc.execute(\"insert or ignore into log_line_identifiers (line_ID, identifier_ID) values (?,?)\", (l_id, i_id))\n",
    "            except IndexError:\n",
    "                pass\n",
    "            line = logfile.readline().casefold()\n",
    "            line_num = line_num + 1\n",
    "        self.__db.commit()\n",
    "        logfile.close()\n",
    "\n",
    "    def export_freq_table(self, filename):\n",
    "        with open(filename, \"w\") as fw:\n",
    "            for r in self.__dbc.execute(\"select token, freq from tokens\"):\n",
    "                fw.write(f\"\\\"{r[0]}\\\", {r[1]}\\n\")\n",
    "\n",
    "    def export_words_table(self, filename):\n",
    "        self.__db.commit()\n",
    "        self.__dbc.execute(f\"attach database '{filename}' as words_db\")\n",
    "        self.__dbc.execute(\"create table if not exists words_db.words (id integer primary key, word text unique not null)\")\n",
    "        self.__dbc.execute(\"insert or ignore into words_db.words (word) select token from tokens\")\n",
    "        self.__db.commit()\n",
    "    \n",
    "    def import_words_table(self, filename):\n",
    "        r = self.__dbc.execute(\"create table if not exists words (id integer primary key, word text unique not null)\")\n",
    "        self.__dbc.execute(\"delete from words\")\n",
    "        with open(filename, \"r\") as fr:\n",
    "            for line in fr:\n",
    "                self.__dbc.execute(\"insert or ignore into words (word) values (?)\",(line.rstrip(),))\n",
    "        self.__db.commit()\n",
    "\n",
    "    def get_words_count(self):\n",
    "        r = self.__dbc.execute(\"select count(*) from words\")\n",
    "        return r.fetchone()[0]\n",
    "\n",
    "    def save_db(self, filename):\n",
    "        self.__db.commit()\n",
    "        self.__dbc.execute(f\"vacuum into '{filename}'\")\n",
    "\n",
    "    def db_handle(self):\n",
    "        return self.__db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the raw dictionary with token counts\n",
    "#logfile = LogFile(\"journal-aws-efs-operator-e2e.0\", 'efs-e2e.db')\n",
    "logfile = LogFile(\"journal-aws-efs-operator-e2e.0\")\n",
    "logfile.parse_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = logfile.db_handle()\n",
    "cur = con.cursor()\n",
    "r = cur.execute(\"select count(ID) from log_lines\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from log_lines limit 20\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cur.execute(\"select count(ID) from tokens\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from tokens order by freq desc limit 20\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile.export_freq_table(\"frequencies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cur.execute(\"select count(id) from identifiers\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from identifiers\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cur.execute(\"select count(line_ID) from log_line_identifiers\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from log_line_identifiers order by line_ID limit 20\"):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cur.execute(\"select count(line_ID) from log_line_tokens\")\n",
    "print(r.fetchone()[0])\n",
    "for r in cur.execute(\"select * from log_line_tokens order by token_ID limit 20\"):\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile.export_words_table('words.db')\n",
    "r = cur.execute(\"create table words (id integer primary key, word text unique not null)\")\n",
    "\n",
    "# Here: review the db / words table and export as csv to be imported as the label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "logfile = LogFile(\"journal-aws-efs-operator-e2e.0\", 'test2.db')\n",
    "logfile.import_words_table('words.csv')\n",
    "logfile.parse_log()\n",
    "\n",
    "con = logfile.db_handle()\n",
    "cur = con.cursor()\n",
    "\n",
    "words_num = logfile.get_words_count()\n",
    "print(f\"Dictitonary size: {words_num} words\")\n",
    "\n",
    "prev_identifier_id = 0\n",
    "prev_line_id = 0\n",
    "raw_dataset = []\n",
    "line_words = torch.zeros(words_num)\n",
    "raw_dataset.append(line_words)\n",
    "for r in cur.execute(\"select words.id as word_id, log_line_identifiers.identifier_ID, log_line_identifiers.line_ID from words \\\n",
    "        join tokens on tokens.token = words.word \\\n",
    "            join log_line_tokens on log_line_tokens.token_ID = tokens.ID \\\n",
    "                join log_line_identifiers on  log_line_identifiers.line_ID = log_line_tokens.line_ID \\\n",
    "                        order by log_line_identifiers.identifier_ID, log_line_identifiers.line_ID, word_id\"):\n",
    "    (word_id, identifier_id, line_id) = r\n",
    "    if line_id != prev_line_id:\n",
    "        raw_dataset.append(line_words)\n",
    "        line_words = torch.zeros(words_num)\n",
    "        prev_line_id = line_id\n",
    "    line_words[word_id - 1] = 1.0\n",
    "training_set = torch.stack(raw_dataset)\n",
    "\n",
    "print(f\"Training set shape: {training_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=1171, hidden_dim=256, num_layers=2, dropout=0.2):\n",
    "        super(LogLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layer to predict next vector\n",
    "        self.output_layer = nn.Linear(hidden_dim, input_dim)\n",
    "        self.sigmoid = nn.Sigmoid()  # For binary prediction (0 or 1 for each word)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_dim)\n",
    "        # LSTM output\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Take the last output from the sequence to predict the next vector\n",
    "        # lstm_out shape: (batch_size, sequence_length, hidden_dim)\n",
    "        last_output = lstm_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Predict next vector\n",
    "        prediction = self.output_layer(last_output)  # (batch_size, input_dim)\n",
    "        prediction = self.sigmoid(prediction)  # Apply sigmoid for binary prediction\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"Initialize hidden states\"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return h0, c0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences of length 50 for training\n",
    "def create_sequences(data, sequence_length=50):\n",
    "    \"\"\"\n",
    "    Create sequences of length sequence_length from the data\n",
    "    Returns X (input sequences) and y (target vectors)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # Input: sequence of 50 vectors\n",
    "        sequence = data[i:i + sequence_length]\n",
    "        # Target: the next vector after the sequence\n",
    "        target = data[i + sequence_length]\n",
    "        \n",
    "        X.append(sequence)\n",
    "        y.append(target)\n",
    "    \n",
    "    return torch.stack(X), torch.stack(y)\n",
    "\n",
    "# Create sequences from training data\n",
    "sequence_length = 50\n",
    "print(f\"Creating sequences of length {sequence_length} from training set...\")\n",
    "X, y = create_sequences(training_set, sequence_length)\n",
    "\n",
    "print(f\"Input sequences shape: {X.shape}\")  # Should be (num_sequences, 50, 1171)\n",
    "print(f\"Target vectors shape: {y.shape}\")   # Should be (num_sequences, 1171)\n",
    "print(f\"Number of training sequences: {len(X)}\")\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = LogLSTM(input_dim=1171, hidden_dim=256, num_layers=2, dropout=0.2)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Manual train/validation split function (no sklearn needed)\n",
    "def manual_train_test_split(X, y, test_size=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Manually split data into train and test sets\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    \n",
    "    # Get total number of samples\n",
    "    total_samples = len(X)\n",
    "    test_samples = int(total_samples * test_size)\n",
    "    \n",
    "    # Create random indices\n",
    "    indices = torch.randperm(total_samples)\n",
    "    \n",
    "    # Split indices\n",
    "    test_indices = indices[:test_samples]\n",
    "    train_indices = indices[test_samples:]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = manual_train_test_split(\n",
    "    X, y, test_size=0.2, random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} sequences\")\n",
    "print(f\"Validation set: {X_val.shape[0]} sequences\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "print(\"Training setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Train the LSTM model\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict_next_vector(model, input_sequence, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict the next vector given an input sequence of 50 vectors\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        input_sequence: Tensor of shape (50, 1171) - sequence of 50 vectors\n",
    "        threshold: Threshold for binary prediction (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Predicted binary vector of shape (1171,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Add batch dimension: (1, 50, 1171)\n",
    "        input_batch = input_sequence.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model(input_batch)  # Shape: (1, 1171)\n",
    "        \n",
    "        # Apply threshold to get binary prediction\n",
    "        # binary_prediction = (prediction > threshold).float()\n",
    "        \n",
    "        return prediction.squeeze(0).cpu()  # Remove batch dimension and move to CPU\n",
    "\n",
    "# Test prediction on a sample\n",
    "if len(X_val) > 0:\n",
    "    # Take the first validation sequence\n",
    "    sample_sequence = X_val[0]  # Shape: (50, 1171)\n",
    "    true_next = y_val[0]        # Shape: (1171,)\n",
    "    \n",
    "    # Predict\n",
    "    predicted_next = predict_next_vector(model, sample_sequence)\n",
    "    \n",
    "    print(\"Sample prediction:\")\n",
    "    print(f\"True next vector (first 10 elements): {true_next[:10].numpy()}\")\n",
    "    print(f\"Predicted next vector (first 10 elements): {predicted_next[:10].numpy()}\")\n",
    "    \n",
    "    # Calculate accuracy for this sample\n",
    "    accuracy = (predicted_next - true_next).abs().float().mean()\n",
    "    print(f\"Prediction accuracy for this sample: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_dim': 1171,\n",
    "        'hidden_dim': 256,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.2\n",
    "    },\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses\n",
    "}, 'log_lstm_model.pth')\n",
    "\n",
    "print(\"\\\\nModel saved as 'log_lstm_model.pth'\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"Load a saved model\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    config = checkpoint['model_config']\n",
    "    model = LogLSTM(**config)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# loaded_model, checkpoint = load_model('log_lstm_model.pth')\n",
    "# \n",
    "# # Use the loaded model for prediction\n",
    "# sample_sequence = training_set[1000:1050]  # Take 50 consecutive vectors\n",
    "# prediction = predict_next_vector(loaded_model, sample_sequence)\n",
    "# print(f\"Prediction shape: {prediction.shape}\")\n",
    "# print(f\"Number of predicted active words: {prediction.sum().item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
